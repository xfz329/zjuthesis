\chapter{基于多维度特征的P子痫前期甄别模型的建立与分析}
\section{引言}
此前的章节已经完成了机器学习任务的所需的数据的所有预处理，本章开始将使用真正的机器学习模型与相关算法开始分析工作。
本章将具体介绍利用前文提出的脉搏波的特征集合、使用机器学习的监督学习与无监督学习的经典算法
构建并评估子痫前期的一般识别模型。此外，本章也对评估机器学习模型性能表现的一般指标进行了介绍。
\section{机器学习及评估}
\subsection{机器学习简介}
一般认为，机器学习是一门致力于研究通过计算的手段、利用已有的经验来改善系统自身性能的学科（和艺术）\cite{Zhou2016,Aurélien2018}。其中，Tom Mitchell对机器学习给出了一种最为经典的形式化定义:
计算机程序利用经验$E$学习任务$T$，其性能是$P$，如果针对任务$T$的性能$P$随着经验$E$不断增长，那么我们就说关于$T$与$P$，该程序对$E$进行了学习，这一过程如\autoref{fig:etp}所示\cite{mitchell1997,Zhou2016}。
对计算机程序而言，$E$通常以数据的形式存在，因此机器学习也可以看成从相关数据中产生模型的算法过程，不显式编程是机器学习最典型的特征。
本文末附录D给出了部分机器学习领域常见的术语及其解释。
\begin{figure}[htbp]
  \centering
  \includegraphics[width=.5\linewidth]{features/etp}
  \caption[机器学习方法的形式化定义]{\label{fig:etp}机器学习的形式化定义}
\end{figure}
尽管机器学习的相关概念早在上世纪五十年代就已经被提出，但直到进入新世纪后，机器学习才真正迎来井喷式发展的黄金期。在过去二十年中，由于半导体电子计算机行业的充分发展，人类收集、传输、处理数据的能力取得了长足的进步，
人类各种社会活动中出现的海量数据具备了能够被挖掘、分析的硬件基础与需要被分析并加以利用的客观需求。在此背景下，机器学习受到了学者们的广泛关注并进入了蓬勃发展阶段，不论是理论基础方面亦或是应用研究方面都
得到了巨大的发展，取得了重大突破。目前，机器学习技术已经被成功应用在模式识别、数据挖掘、自然语言处理、语言识别、图像识别、芯片设计、信息检索及生物信息学等学科领域，
尤其是为交叉学科的发展研究提供了新的技术支撑与突破点\cite{Zhou2016,Aurélien2018,Li2017}。
\subsection{监督学习的评价指标}
监督学习是一种用于推断观察数据（也称为输入数据）与目标变量（因变量或标签）之间的潜在关系的学习机制。
学习任务使用标记的训练数据（训练示例）来合成模型函数，这些模型函数的目的是概括特征向量（输入）和监控信号（输出）之间的基本关系。
训练数据包括观察到的输入（特征）向量和期望的输出值（也称为监控信号或类别标签）。
基于监督学习算法的经过良好训练的函数模型可以准确预测隐藏在不熟悉或未观察到的数据实例中的隐藏现象的类标签。

一、混淆矩阵

混淆矩阵（confusion matrix）是评估分类器分类效果优劣的常用工具\cite{Zhou2016,Aurélien2018}。其总体思路就是分别统计A类别实例被划分成B类别实例的数目。理论上混淆矩阵的行列没有上限，而在实际应用中，二分类任务的混淆矩阵是最常见的。
此时，将样例依据其真实所属类别与分类器预测类别进行组合可得到四种结果：真阳性（true positive，TP）、假阳性（false positive，FP）、真阴性（true negative，TN）及假阴性（false negative，TN）,如\autoref{tab:cm}所示。此时显然有
$TP+FP+TN+FN=\text{样例总数}$。
\begin{table}[htbp]
      \centering
      \caption{\label{tab:cm}二分类任务的混淆矩阵}
      \begin{tabular}{ccc}
      \toprule
      \multicolumn{1}{c}{\multirow{2}[4]{*}{\textbf{真实情况}}} & \multicolumn{2}{c}{\textbf{预测结果}} \\
            \cmidrule{2-3}          & 阳性（1） & 阴性（0） \\
      \midrule
      阳性（1） & 真阳性（TP） & 假阴性（FN） \\
      阴性（0） & 假阳性（FP） & 真阴性（TN） \\
      \bottomrule
      \end{tabular}%
\end{table}%

为量化分类器的具体性能，人们在混淆矩阵的基础上衍生定义了一系列数字指标，包括查全率（recall）、查准率(precison)、准确率（accuracy)及特异性(specificity)等，如\autoref{equ:measures}所示。
\begin{equation}
      \label{equ:measures}
      \left \{
      \begin{aligned}
            Recall      &=\frac{TP}{TP+FN}         \\
            Precison    &=\frac{TP}{TP+FP}          \\
            Accuracy    &=\frac{TP+TN}{TP+FP+TN+FN} \\
            Specificity &=\frac{TN}{TN+FP}       \\
      \end{aligned}
      \right.
\end{equation}
其中，查全率亦称召回率、灵敏性（sensitivity）或真阳性率（true positive rate，TPR），查准率亦称精准率，特异性亦称真阴性率。查全率与查准率是应用的最广泛的两个指标\cite{Zhou2016,Aurélien2018}。
一般而言，查全率与查准率是对相互矛盾的度量指标，一个指标性能的提高意味着另一个指标性能的下降。通常只有在简单分类任务中，
才能同时获得较高的查准率与查全率。这称为精度-召回率权衡。为评估查全率与查准率均不相等的分类器性能，人们进一步定义了$F_1\text{分数}$，如\autoref{equ:f1}所示。
\begin{equation}
      \label{equ:f1}
      F_1=\frac{2}{\frac{1}{Precison}+\frac{1}{Recall}}=\frac{2\cdot Precison\cdot Recall}{Precison+Recall}=\frac{TP}{TP+\frac{FN+FP}{2}}
\end{equation}
$F_1\text{分数}$是召回率与精准率的谐波均值。召回率与精准率相近的分类器易获得更高的$F_1\text{分数}$。

在评估分类器性能时需要根据场景，从\autoref{equ:measures}与\autoref{equ:f1}中灵活选取恰当的评价指标。

二、ROC曲线、AUC与约登指数

受试者工作特征（Receiver Operating Characteristic，ROC）曲线是另一种常用于二分类问题的分析工具。ROC绘制的是真阳性率和假阳性率（false positive rate，FPR）之间的变化关系，其中
\begin{equation}
      \label{equ:fpr}
      FPR=\frac{TN}{TN+FP}=1-Specificity
\end{equation}
因此，ROC曲线也被称为灵敏度与1-特异性曲线。绘制曲线时，以分类器的预测结果对样例进行升序排列，依次将样本作为阳性进行预测，计算对应的TPR与FPR后，可得一坐标点$({FPR}_i,{TPR}_i)$，最后将所有坐标点连线即可，如\autoref{fig:roc}所示。
其中，虚线表示纯随机分类器的ROC曲线，理想性能的分类器应无限逼近左上角，即坐标点$(0,1)$。
\begin{figure}[htbp]
      \centering
      \includegraphics[width=.6\linewidth]{data_plan/roc}
      \caption[ROC曲线与AUC数值]{\label{fig:roc}ROC曲线与AUC数值。各分类器的AUC具体数值参见图例。}
\end{figure}

在衡量多个分类器性能优劣时，常将分类器对应的ROC曲线下面积作为判据，即为AUC（Area Under Curve）。纯随机分类器ROC的AUC数值为0.5，而理想分类器ROC的AUC数值为1，如\autoref{fig:roc}所示。

此外，约登指数（Youden Index）也是用来评价分类器效果的一个指标。若在评估分类器性能时，给予将分类器假阴性和假阳性以相同权重，即可应用约登指数
\begin{equation}
      \label{equ:yi}
      \begin{aligned}
            YI&=Sensitivity-(1-Specificity)\\
            &=Sensitivity+Specificity-1
      \end{aligned}
\end{equation}
一般认为，当YI取值最大时，此时对应的分类阈值为最佳阈值\cite{cwl}。
\section{监督学习算法的具体表现及分析}
\subsection{使用的主要算法}
决策树（Decision Tree，DT）是数据挖掘的经典算法之一，是一种类似流程图的树结构，可以用于连续数值型变量的回归预测及离散型数值型变量的分类问题\cite{Li2017,Liu2018}。
决策树算法的最显著优点是简单直观，易于可视化、可读性强。

一、决策树

1、决策树的结构与原理

分类决策树模型是对实例进行分类的树形结构的描述，如\autoref{fig:dt}所示。一般而言，决策树由结点和有向边组成，而结点又可分为内部结点与叶节点。其中，前者表示一个特征或属性，后者对应决策结果，一般是一个类\cite{Li2017,Zhou2016}。
决策树可以看成一个if-else规则的集合，决策树的根节点到叶节点的每条路径分别对应着一条规则：路径上的内部结点集合构成了规则的判断条件，而叶节点所属的具体类则对应着该规则的结论。
决策树的学习目的就是从训练数据集中集中归纳出一组分类规则，产生一颗与训练数据的矛盾小的、泛化能力强的逻辑判断树。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{models/dt.png}
    \caption{\label{fig:dt}决策树模型示意}
\end{figure}

2、决策树的特征选择

为生成一颗分类能力强的决策树，一种可行的策略是只选取对分类效果有提升的特征参与构建。基尼指数（Gini index）与信息增益（information gain）是两种最常用的用于筛选最佳特征的指标。

基尼指数,也称为基尼不纯度，其定义为
\begin{equation}
    \label{equ:gini}
    G_i = 1 - \sum_{k=1}^n{p_{i,k}}^2
\end{equation}
其中，$p_{i,k}$是第$i$个节点上，类别为$k$的训练实例占比。基尼指数数值越大，样本集合的不确定性也越大。特别地，若样本集合$D$根据特征$A$是否取某一可能值$a$而被分割成$D_1$和$D_2$两部分，即
\begin{equation}
    \label{equ:daset}
    \left \{
    \begin{aligned}
        D_1 &= \{ (x,y) \in D \mid A(x) = a\} \\
        D_2 &= D - D_1
    \end{aligned}
    \right.
\end{equation}
那么，在特征$A$的条件下，样本集合的基尼指数可以表示为
\begin{equation}
    \label{equ:ginia}
    G(D,A) = \frac{|D_1|}{|D|}G(D_1) + \frac{|D_2|}{|D|}G(D_2)
\end{equation}
其中，$|D|$表示样本集合$D$的样本数量。\autoref{equ:ginia}描述了$D$经特征$A=a$分割后的不确定性，此时，筛选最佳特征的过程可以转换为寻找使\autoref{equ:ginia}取值最小的特征$A$的具体数值。

另一方面，信息增益是在引入了信息论中的信息熵（information entropy）概念进行定义的并计算使用的，其作用与具体使用方法与基尼指数类似，这里不再进行赘述\cite{Zhou2016,Li2017}。

3、决策树的生成

决策树的生成构建过程就是递归地选择最优特征，并根据最优特征对训练数据进行分割，使该分割对各个新的子数据集有最优分类效果的过程。其中，最经典生成算法包括ID3（Iterative Dichotomiser 3，第三代迭代二分器）决策树学习算法、
C4.5（Classifier 4.5，第4.5代分类器）决策树学习算法及CART（classification and regression tree，分类与回归树）决策树学习算法等三种\cite{quinlan1986,quinlan1993,breiman1984}。
在这三种算法中，只有CART算法生成的决策树既可以执行分类任务也可以执行回归任务，故CART算法的应用也最为广泛。

CART算法采用最小基尼指数来选择特征，生成的决策树为二叉树。其工作的基本原理如\autoref{alg:cart}所示。
\begin{breakablealgorithm}
    \caption[CART生成算法]{CART递归生成算法\cite{Li2017}}
    \label{alg:cart}
    \begin{algorithmic}[1] %每行显示行号
        \Require 训练数据集$D$。
        \Ensure CART决策树。
        \State 建立一颗空树$CART$，设该树的根结点为$root$。
        \Function{GenerateCart}{$CART,D_c,root$}
            \State $D_c$为当前结点的训练数据集，计算现有特征对该数据集的基尼指数。对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D_c$分割成$D_l$与$D_r$两部分，使用\autoref{equ:ginia}计算$A=a$时的基尼指数。
            \State 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最有特征与最优切分点。
            \State 依最优特征与最优切分点，从现结点生成两个子节点$left$与$right$，将训练数据集依特征分配到两个子节点中去，即$D_l$与$D_r$。更新当前$CART$。
            \If {结点中的样本个数小于预定阈值 \textbf{or} 样本集的基尼指数小于预定阈值 \textbf{or} 没有更多特征}
                \State \Return{$CART$}
            \Else    
                \State \Call{GenerateCart}{$CART,D_l,left$}
                \State \Call{GenerateCart}{$CART,D_r,right$}
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{breakablealgorithm}

4、决策树的剪枝

为防止决策树出现过拟合的情况，通常都会对过于“茂密”的树进行剪枝（pruning）处理。决策树的剪枝方法可以分为预剪枝与后剪枝两大类。
前者的工作原理是在决策树的生长阶段就对其进行一定的限制，包括限制树最大生长深度、限制决策树生成的最多叶节点数量等。
后剪枝则是在决策树得到完全生长之后进行，其处理算法也更复杂、训练时间等开销也更大\cite{Zhou2016,Liu2018}。

二、Bagging与随机森林

1、Bagging

为获得泛化性能强的不同的基学习器，一种可行的策略是对所有学习器使用同一种训练算法，但是在训练集的不同随机子集上进行训练，使这些基学习器的能够有一定的差异。
\autoref{fig:bp}所示，在上述过程的随机子集的建立过程中，若对原始数据样本的采样后放回，这种方法即为bagging；与之对应的，采样后不放回的方法称为pasting\cite{Aurélien2018,Zhou2016}。
由于bagging方法可以获得的随机子集数量要远远高于pasting方法，bagging方法应用得也更加广泛。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{models/bp}
    \caption[Bagging与Pasting示意]{\label{fig:bp}Bagging与Pasting示意\cite{Aurélien2018}}
\end{figure}

当使用有采样后即放回的方法从包含$m$个原始样本的训练集$D$抽取出$m$个新的样本构成此次训练数据集$D_{bs}$时，显然，$D$有部分数据样本在$D_{bs}$重复出现，部分数据则从未被抽样过。
易知样本在$m$次均未被抽中的概率为$(1-\frac{1}{m})^m$，当$m \to \infty$时，
\begin{equation}
    \label{equ:me}
    \lim_{m \to \infty}{(1-\frac{1}{m})}^m = \frac{1}{e} \approx 0.368
\end{equation}
\autoref{equ:me}说明约有36.8\%的原始样本未出现在采样集$D_{bs}$中，这部分数据可以作为当前训练算法的测试集。

按上述思想，可从原始样本训练集$D$采样得到$T$个包含$m$个训练样本的采样集，基于这些采样集，使用特定的机器学习算法可以训练得到$T$个基学习器，结合这些基学习器的输出即为Bagging算法
的基本流程，如\autoref{alg:bagging}所示。投票法和平均法是Bagging在进行基学习器的输出时常采用的策略。
\begin{breakablealgorithm}
    \caption[Bagging算法]{Bagging算法\cite{Zhou2016}}
    \label{alg:bagging}
    \begin{algorithmic}[1] %每行显示行号
        \Require 训练集$D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}$；基学习算法$\xi$；训练轮数$T$。
        \Ensure $H(x)=\arg \max \limits_{y \in Y} \sum_{t=1}^T \mathbb{I}(h_t(x)=y)$，其中$\mathbb{I}(.)$为指示函数，在$.$为真或假时函数值分别为1或0。
        \For {$t=1,2,\dots,T$}
            \State 从原始训练集$D$自助采样得到此次的样本分布$D_{bs}$
            \State $h_t=\xi (D,D_{bs})$
        \EndFor
    \end{algorithmic}
\end{breakablealgorithm}

2、随机森林

随机森林是另一种在Bagging算法基础上发展起来的集成学习的算法，最早由Leo Breiman于2001年提出\cite{breiman2001}。如\autoref{fig:rf}所示，“森林”表示该算法是由多棵\autoref{fig:dt}所示的决策树构成，
这些决策树一般都是经过充分生长的、未经剪枝处理的CART决策树。而“随机”一词有两重含义，首先是同Bagging算法一样，每棵决策树在训练时使用的训练样本是随机抽取的；其次，与\autoref{alg:cart}所示的一般CART决策树生成算法不同，随机森林中的CART
决策树在生长时并不是在当前结点的$d$个属性集合$A$中选取最优特征及其最优切分点，而是先从$A$中随机生成一个包含$k$个属性子集的$A_{bs}$，随后再从$A_{bs}$中选择最优属性进行划分\cite{Zhou2016,Liu2018,breiman2001}。其中，$k$的推荐取值为
$\lfloor \log_2m + 1 \rfloor$\cite{breiman2001}。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{models/rf}
    \caption{\label{fig:rf}随机森林示意}
\end{figure}

对回归问题而言，随机森林算法的输出是所有决策树的输出的均值；而对分类任务而言，算法输出是所有决策树输出投票的结果。随机森林在树的生长过程中的两次随机使决策树具有更大的多样性，相当于用更高的偏差换取更低的方差。因此，
最终的集成结果有着出色的泛化性能，有效避免了单决策树可能导致的过拟合问题。随机森林算法运行速度快、准确率高且泛化性能优秀，被誉为“代表集成学习技术水平的方法”\cite{Zhou2016,Liu2018}。

此外，随机森林算法往往也会在特征选择的过程中得到应用\cite{Aurélien2018}。重新考察\autoref{fig:dt}中的决策树可以发现，越靠近根结点位置的特征对决策过程的重要程度也越高，而不重要的特征多出现在靠近结点的位置、甚至不出现在决策树中。
因此，特征的重要性（或贡献度）可以通过计算其在随机森林众多决策树的平均深度来进行量化衡量。
\subsection{基于脉搏波时域特征集\Rnum{1}的结果及分析}
一、按照全部波形抽样

1. 模型初筛

为检验借助脉搏波时域特征集\Rnum{1}中的各项时域参数能否识别出孕妇是否患有子痫前期，本研究首先基于全部波形分层抽样的数据样本进行了监督学习算法的试探性研究\cite{scikit-learn}。
本研究共使用了随机梯度下降、决策树、K近邻、高斯朴素贝叶斯算法、逻辑回归、线性支持向量机、核支持向量机、C-支持向量机及多层感知机等9种基本分类算法进行模型探究。这些模型在进行初筛时，模型超参数均使用了默认设置\cite{scikit-learn}。
9种模型的初筛结果如\autoref{tab:model_screen}所示，其中训练集相关数据是对原始训练集数据经过5层交叉验证后
\begin{landscape}
      \zihao{-5}
      \begin{longtable}{m{3cm}<{\centering}m{1.7cm}<{\centering}m{2.3cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}}
            \caption{初筛结果}\\
            \label{tab:model_screen}\\
            \toprule
            &  & \multicolumn{6}{c}{\textbf{训练集（5层交叉验证）}} & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                      \\
            \multirow{-2}{*}{\textbf{模型类型}} & \multirow{-2}{*}{\textbf{训练时间}} & \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率} &  \textbf{AUC} &  \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率}    \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            &  & \multicolumn{6}{c}{\textbf{训练集（5层交叉验证）}} & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                      \\
            \multirow{-2}{*}{\textbf{模型类型}} & \multirow{-2}{*}{\textbf{训练时间}} & \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率} &  \textbf{AUC} &  \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率}    \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            随机梯度下降      &   6.17 s  &     $\left[ \begin{array}{cc} 2165 & 380 \\ 1164 & 2582 \end{array} \right]$ & 87.2\% & 68.9\% & 77.0\% & 75.5\% & 0.876 &
            $\left[ \begin{array}{cc} 328 & 308 \\ 22 & 915 \end{array} \right]$ & 74.8\% & 97.7\% & 84.7\% & 79.0\% \\
            决策树            &   5.24 s  &     $\left[ \begin{array}{cc} 2241 & 304 \\ 635 & 3111 \end{array} \right]$ & 91.1\% & 83.0\% & 86.9\% & 85.1\% & 0.907 &
            $\left[ \begin{array}{cc} 584 & 52 \\ 174 & 763 \end{array} \right]$ & 93.6\% & 81.4\% & 87.1\% & 85.6\% \\
            K近邻算法      &   3.08 s  &     $\left[ \begin{array}{cc} 2347 & 198 \\ 237 & 3509 \end{array} \right]$ & 94.7\% & 93.7\% & 94.2\% & 93.1\% & 0.974 &
            $\left[ \begin{array}{cc} 594 & 42 \\ 63 & 874 \end{array} \right]$ & 95.4\% & 93.3\% & 94.3\% & 93.3\% \\
            高斯朴素贝叶斯算法      &   1.22 s  &     $\left[ \begin{array}{cc} 2215 & 320 \\ 1354 & 2392 \end{array} \right]$ & 87.9\% & 63.9\% & 74.0\% & 73.2\% & 0.838 &
            $\left[ \begin{array}{cc} 569 & 67 \\ 328 & 609 \end{array} \right]$ & 90.1\% & 65.0\% & 75.5\% & 74.9\% \\
            逻辑回归算法      &   203.0 s  &     $\left[ \begin{array}{cc} 2174 & 371 \\ 347 & 3399 \end{array} \right]$ & 90.2\% & 90.7\% & 90.4\% & 88.6\% & 0.950 &
            $\left[ \begin{array}{cc} 573 & 63 \\ 66 & 871 \end{array} \right]$ & 93.3\% & 93.0\% & 93.1\% & 91.8\% \\
            线性支持向量机      &   47.22 s  &     $\left[ \begin{array}{cc} 1658 & 887 \\ 254 & 3492 \end{array} \right]$ & 79.7\% & 93.2\% & 86.0\% & 81.9\% & 0.917 &
            $\left[ \begin{array}{cc} 603 & 33 \\ 170 & 767 \end{array} \right]$ & 95.9\% & 81.9\% & 88.3\% & 87.1\% \\
            核支持向量机      &   60.28 s  &     $\left[ \begin{array}{cc} 1828 & 717 \\ 363 & 3383 \end{array} \right]$ & 82.5\% & 90.3\% & 86.2\% & 82.8\% & 0.916 &
            $\left[ \begin{array}{cc} 484 & 152 \\ 81 & 856 \end{array} \right]$ & 84.9\% & 91.4\% & 88.0\% & 85.2\% \\
            C-支持向量机      &   42.47 s  &     $\left[ \begin{array}{cc} 1914 & 631 \\ 354 & 3392 \end{array} \right]$ & 84.3\% & 90.5\% & 87.3\% & 84.3\% & 0.929 &
            $\left[ \begin{array}{cc} 510 & 126 \\ 86 & 851 \end{array} \right]$ & 87.1\% & 90.8\% & 88.9\% & 86.5\% \\
            多层感知机      &   26.8 s  &     $\left[ \begin{array}{cc} 1982 & 563 \\ 906 & 2840 \end{array} \right]$ & 83.5\% & 75.8\% & 79.5\% & 76.6\% & 0.905 &
            $\left[ \begin{array}{cc} 534 & 102 \\ 83 & 854 \end{array} \right]$ & 89.3\% & 91.1\% & 90.2\% & 88.2\% \\
      \end{longtable}
\end{landscape}
\leftline{得到的。}

从\autoref{tab:model_screen}中结果可以得到以下结论:

\Rnum{1} 在测试集上，除高斯朴素贝叶斯算法外剩余8种模型的AUC数值均在0.850以上，其中，K近邻算法的AUC数值更是高达0.974。
从各模型在训练集上得到的混淆矩阵来看，K近邻算法与逻辑回归算法在精度-召回率权衡上表现最好，精确率、召回率及F1数值均在90.0\%以上。决策树算法与三种支持向量机算法在精确率与召回率可以达到90.0\%+80.0\%
（或80.0\%+90.0\%）以上，4种算法模型的F1值也均在86.0\%以上。而剩下的随机梯度算法、高斯朴素贝叶斯算法与多层感知机算法在这些数值上表现较差。
另外，在各模型的训练时间方面，高斯朴素贝叶斯算法训练所需时间最短，仅需1.22s，而多层感知机、支持向量机模型所需时间较长、逻辑回归算法训练时间最长为203.0s。这些数值也与各算法模型的
复杂度对应，符合预期。

\Rnum{2} 在验证集上，随机梯度算法与高斯朴素贝叶斯算法的表现最差，出现精确率或召回率数值小于75\%的情况。剩余7种算法均在验证集上有较好的泛化能力，决策树算法、逻辑回归算法、三种支持向量机算法及多层感知机算法性能接近，精确率与召回率可以达到90.0\%+80.0\%
（或80.0\%+90.0\%），F1值也均在87.0\%以上。而K近邻算法与逻辑回归算法表现最为优秀，精确率、召回率与F1值三者数值更是均在93.0\%以上。

综上，上述结果初步说明了本研究提出的脉搏波时域特征集\Rnum{1}在子痫前期的识别问题上具有较强的表征能力，可以构建出具有较好泛化性能的子痫前期识别分类模型。
若不考虑模型训练所需时间，K近邻算法与逻辑回归算法的表现最为出色，随机梯度算法与高斯朴素贝叶斯算法表现最差。

2. 超参数优化

上小节已经在脉搏波时域特征集\Rnum{1}上初步得到了各机器学习模型的表现。由于上述模型的建立均采用默认超参数，上小节的结论并不完全严谨。
一般而言，超参数的调整与优化会使模型的性能得到提升。因此，本小节主要对模型超参数的设置进行了研究。
在综合考虑模型的训练时间及初筛时的性能表现，本研究从上述9种单一分类中选取了随机梯度下降算法、高斯朴素贝叶斯算法、决策树算法与K近邻算法等四种模型进行了超参数调优工作。
其中，前两类在初筛阶段表现较差的算法主要探索能否通过超参数的调整提升性能，后两种算法的超参数调整则是为了进一步的考察性能。
各模型超参数的最优数值组合通过网格搜索的方法来进行探索，而优化的评价标准是新生成模型在训
\begin{landscape}
      \zihao{-5}
      \begin{longtable}{m{3cm}<{\centering}m{5cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{3cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}}
            \caption{超参数优化前后模型性能对比}\\
            \label{tab:super_para}\\
            \toprule
            \multicolumn{1}{c}{\multirow{3}{*}{\textbf{模型类型}}} & \multicolumn{1}{c}{\multirow{3}{*}{\textbf{超参数组合值域}}}    & \multicolumn{3}{c}{\textbf{优化前}}   & \multicolumn{1}{c}{\multirow{3}{*}{\textbf{最优超参数}}}   & \multicolumn{3}{c}{\textbf{优化后}} \\
            \multicolumn{1}{c}{} & \multicolumn{1}{c}{}     & \textbf{训练集} & \multicolumn{2}{c}{\textbf{测试集}} & \multicolumn{1}{c}{}   & \textbf{训练集} & \multicolumn{2}{c}{\textbf{测试集}}     \\
            \multicolumn{1}{c}{} & \multicolumn{1}{c}{}     & \textbf{AUC} & \textbf{混淆矩阵}    & \textbf{准确率} & \multicolumn{1}{c}{}    & \textbf{AUC} & \multicolumn{1}{c}{\textbf{混淆矩阵}}     & \textbf{准确率} \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            \multicolumn{1}{c}{\multirow{3}{*}{\textbf{模型类型}}} & \multicolumn{1}{c}{\multirow{3}{*}{\textbf{超参数组合值域}}}    & \multicolumn{3}{c}{\textbf{优化前}}   & \multicolumn{1}{c}{\multirow{3}{*}{\textbf{最优超参数}}}   & \multicolumn{3}{c}{\textbf{优化后}} \\
            \multicolumn{1}{c}{} & \multicolumn{1}{c}{}     & \textbf{训练集} & \multicolumn{2}{c}{\textbf{测试集}} & \multicolumn{1}{c}{}   & \textbf{训练集} & \multicolumn{2}{c}{\textbf{测试集}}     \\
            \multicolumn{1}{c}{} & \multicolumn{1}{c}{}     & \textbf{AUC} & \textbf{混淆矩阵}    & \textbf{准确率} & \multicolumn{1}{c}{}    & \textbf{AUC} & \multicolumn{1}{c}{\textbf{混淆矩阵}}     & \textbf{准确率} \\
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            % \multirow{2}{*}{随机梯度下降}                            & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}loss:{[}hinge, log\_loss,   \\ log, modified\_huber, \\ squared\_hinge, perceptron, \\ squared\_error,  huber,\\  epsilon\_insensitive, \\ squared\_epsilon\_insensitive{]},\\    penalty:{[}l2,l1,elasticnet{]},\\   alpha:{[}0.001,0.0001,0.00001{]}\end{tabular}} & \multirow{2}{*}{0.876} & \multirow{2}{*}{$\left[ \begin{array}{cc} 328 & 308 \\ 22 & 915 \end{array} \right]$ } & \multirow{2}{*}{79.0\%} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}alpha=0.001, \\ loss=squared\_hinge,   \\ penalty=elasticnet\end{tabular}} & \multirow{2}{*}{0.918} & \multirow{2}{*}{$\left[ \begin{array}{cc} 631 & 5 \\ 393 & 544 \end{array} \right]$} & \multirow{2}{*}{74.7\%} \\
            % &                      &      &       &    &    &    &      &     \\
            随机梯度下降    & \begin{tabular}[c]{@{}l@{}}loss:{[}\textbf{hinge}, log\_loss,   \\ log, modified\_huber, \\ squared\_hinge, perceptron, \\ squared\_error,  huber,\\  epsilon\_insensitive, \\ squared\_epsilon\_insensitive{]},\\    penalty:{[}\textbf{l2},l1,elasticnet{]},\\   alpha:{[}0.001,\textbf{0.0001},0.00001{]}\end{tabular} & 0.876        & $\left[ \begin{array}{cc} 328 & 308 \\ 22 & 915 \end{array} \right]$ & 79.0\%       & \begin{tabular}[c]{@{}l@{}}alpha=0.001, \\ loss=squared\_hinge,   \\ penalty=elasticnet\end{tabular} & 0.918        & $\left[ \begin{array}{cc} 631 & 5 \\ 393 & 544 \end{array} \right]$ & 74.7\%       \\
            高斯朴素贝叶斯算法   & var\_smoothing:{[}1e-5,1e-7,\textbf{1e-9},1e-11{]}       & 0.838        & $\left[ \begin{array}{cc} 569 & 67 \\ 328 & 609 \end{array} \right]$ & 74.9\%       & var\_smoothing=1e-7,                   & 0.842        & $\left[ \begin{array}{cc} 568 & 68 \\ 328 & 609 \end{array} \right]$ & 74.8\%      \\
            决策树          & \begin{tabular}[c]{@{}l@{}}criterion:{[}\textbf{gini},entropy,log\_loss{]},\\  splitter:{[}\textbf{best},random{]},\\     max\_depth:{[}\textbf{3},4,5{]},\\  max\_features:{[}sqrt,log2,\textbf{None}{]}\end{tabular}       & 0.907        & $\left[ \begin{array}{cc} 584 & 52 \\ 174 & 763 \end{array} \right]$ & 85.6\%       & \begin{tabular}[c]{@{}l@{}}criterion=entropy,\\  max\_depth=5, \\ max\_features=None\end{tabular}                             & 0.949        & $\left[ \begin{array}{cc} 621 & 15 \\ 159 & 778 \end{array} \right]$ & 88.9\%       \\
            K近邻算法           & \begin{tabular}[c]{@{}l@{}}n\_neighbors:{[}3,\textbf{5},7,9{]},\\    weights:{[}\textbf{uniform},distance{]}\end{tabular}     & 0.974        & $\left[ \begin{array}{cc} 594 & 42 \\ 63 & 874 \end{array} \right]$    & 93.3\%       & \begin{tabular}[c]{@{}l@{}}n\_neighbors=9,\\  weights=distance\end{tabular}      & 0.978        & $\left[ \begin{array}{cc} 598 & 38 \\ 67 & 870 \end{array} \right]$ & 93.3\%       \\
      \end{longtable}
\end{landscape}
\leftline{练集上的AUC面积数值大小。}

\autoref{tab:super_para}展示了超参数优化的过程与结果。其中，表格中超参数值域一栏给出了网格搜索时使用的具体超参数及其值域范围，加粗数值为默认超参数数值；优化前后，模型在训练集上的AUC数值均是在
进行了5层交叉验证后取得的；最优超参数一栏则是在各模型在训练集AUC取最大值时使用的超参数组合，此时模型在测试集上的混淆矩阵与准确率也在表格中一并给出。

从\autoref{tab:super_para}中的具体数值不难发现，四种模型的在超参数调整前后的AUC数值均有所提高。但只有决策树算法与K近邻算法
延续了之前的优秀表现，甚至整体性能有微幅上升。但对随机梯度下降算法与高斯朴素贝叶斯算法而言，超参数的调整并没有明显的提升模型性能。
因此，本小节的实验结果进一步佐证了上小节初筛时得到的各项结论。


3. 随机森林算法与特征降维

此前初筛过程中使用的均是单机器学习模型，本小节则使用了集成学习中的随机森林算法在脉搏波时域特征集\Rnum{1}进行了模型训练。
如\autoref{tab:rf_dr_2}第一行所示，使用默认超参数生成的随机森林模型在训练集与测试集上均有着优秀的表现，其在训练集上的AUC面积为0.990，在训练集与测试集上的准确率更是分别达到了95.2\%与97.0\%。
这些数值均高于之前的单机器模型中性能最好的K近邻算法与逻辑回归算法，充分体现了集成学习的优势。

\begin{figure}[htbp]
      \centering
      \includegraphics[width=\linewidth]{results/dt_clf}
      \caption{\label{fig:dt_clf}在训练集上的构建的一棵决策树示意}
\end{figure}

另一方面，本章之前的内容已经阐述过随机森林算法可以用来衡量原始数据样本各属性的贡献度，即随机森林算法可以用作特征降维处理。在使用脉搏波时域特征集\Rnum{1}的286个时域特征完成随机森林的构建之后，可以从中得到各特征对最终模型的贡献度的数值，
结果如\autoref{tab:rf_dr_1}所示。脉搏波时域特征集\Rnum{1}共有的286个特征中，这些特征的平均贡献度为0.34\%，贡献度最高的特征$CVALF\_9$的贡献度为平均值的13.5倍。
若以特征$CVALF\_9$为基准，可以得到所有特征对随机森林模型的相对贡献度如\autoref{fig:rf_importance_pulse}所示。

\begin{center}
      \zihao{5}
      \begin{longtable}{m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}}
            \caption[参与构建随机森林的特征贡献度]{参与构建随机森林的特征贡献度。仅列举了对随机森林累计贡献度达51.9\%的前36个特征。}\\
            \label{tab:rf_dr_1}\\
            \toprule
            \textbf{特征名}&\textbf{贡献度}&\textbf{特征名}&\textbf{贡献度}&\textbf{特征名}&\textbf{贡献度}\\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            \textbf{特征名}&\textbf{贡献度}&\textbf{特征名}&\textbf{贡献度}&\textbf{特征名}&\textbf{贡献度}\\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            CVALF\_9                         & 4.6\%                            & LVRF\_9                          & 3.4\%                            & SVD\_10                          & 3.0\%                            \\
            SVAF\_10                         & 2.5\%                            & LVALF\_7                         & 2.4\%                            & SVAT\_10                         & 2.3\%                            \\
            CVRF\_11                         & 2.1\%                            & SVAR\_10                         & 1.8\%                            & STDZ\_3                          & 1.6\%                            \\
            LVRF\_8                          & 1.6\%                            & LVALF\_6                         & 1.6\%                            & CVD\_11                          & 1.6\%                            \\
            STDZ\_1                          & 1.5\%                            & CVALF\_8                         & 1.5\%                            & SVD\_9                           & 1.4\%                            \\
            SVAAR\_10                        & 1.4\%                            & CVALF\_1                         & 1.4\%                            & SVAAR\_8                         & 1.3\%                            \\
            SVD\_8                           & 1.1\%                            & SVAR\_9                          & 1.0\%                            & SVAF\_9                          & 1.0\%                            \\
            CVAAF\_1                         & 1.0\%                            & LVLF\_8                          & 0.9\%                            & SVAT\_8                          & 0.9\%                            \\
            CVRF\_1                          & 0.9\%                            & CVAAF\_2                         & 0.9\%                            & SVAR\_8                          & 0.8\%                            \\
            SVAF\_2                          & 0.8\%                            & CVRR\_11                         & 0.8\%                            & LVRF\_1                          & 0.7\%                            \\
            SVSR\_7                          & 0.7\%                            & CVRF\_3                          & 0.7\%                            & CVALR\_10                        & 0.7\%                            \\
            CVALF\_7                         & 0.7\%                            & CVAAR\_5                         & 0.7\%                            & LVRF\_2                          & 0.6\%                           
      \end{longtable}
\end{center}

\begin{figure}[htbp]
      \centering
      \includegraphics[width=0.55\linewidth]{results/rf_ip_pulse_0.52}
      \caption[各特征对随机森林的相对贡献度]{\label{fig:rf_importance_pulse}各特征对随机森林的相对贡献度。仅列举了对随机森林累计贡献度达51.9\%的前36个特征。}
\end{figure}

在获取了所有特征对随机森林的贡献度后，此时即可按照贡献度按从高到低的顺序进行特征筛选，新筛选出的特征也被用于建立随机模型用于识别子痫，并用于评价筛选的效果。\autoref{tab:rf_dr_2}展示了这一过程，
其中筛选按所有特征对初始随机森林贡献度的百分比例以10\%为梯度进行，每栏给出了使用筛选出的特征上建立的随机森林的效果。

从\autoref{tab:rf_dr_2}可以发现以下现象。首先，随着特征数量的减少，随机森林模型的训练速度大大加快。其次，随着特征数量的减少，新生成的随机森林模型在训练集与测试集上均出现了性能小幅提高再下降的整体趋势，
模型最佳性能出现在贡献度60\%-70\%之间。这说明筛选刚开始进行时排除了大量无关特征，从而使模型性能得到了提升；而随着筛选的进行，对模型生成有贡献度的特征数量的也在减少，导致随机森林的性能降低。
第三，脉搏波时域特征集\Rnum{1}的286个原始特征中具有子痫前期表征能力的特征较少，存在大量不具任何子痫前期表征能力的冗余特征。在保留原始特征集90\%的贡献度的条件下，就可以
在几乎不损失模型预测能力的条件下排除约43.4\%的无关特征。甚至在仅保留原始特征集30\%的贡献度的13个特征上建立的随机森林模型也有着接近原始随机森林模型的性能（准确率96.3\% VS 97.0\%）。

4. 综合分析

从特征的角度而言，结合\autoref{tab:rf_dr_2}中结果，脉搏波时域特征集\Rnum{1}包含了一定的具有子痫前期表征能力的脉搏波时域形态特征。而研究\autoref{tab:rf_dr_1}中各特征具体的贡献度可以发现，
在这些有效特征中，与下降支相关的特征要远多于上升支的特征。这也与脉搏波下降支是血液回流过程的反应、下降支包含了更多的血液循环中细节信息的事实相符合。
从\autoref{tab:rf_dr_1}的指标种类来看，左视类指标相较中视类指标与分层类指标也更少，这也可能与左视类指标对下降支的表征效果不及另外两类有关。
此外，\autoref{tab:rf_dr_1}中的特征的下标也集中在前端（1-3）与尾端（8-10）,这说明脉搏波下降支的前端
与尾端可能包含了更多与子痫前期相关的信息，如下降支在前端是否快速下降，在尾端是否平缓等。

本小节按照全部波形进行子痫前期识别模型的研究过程实际上基于子痫前期导致的病生理变化可以在单个波形上得到体现的假设。即单个波形也包含了
识别子痫前期的全部信息。本小节的研究结果为这种假设提供了一定的支撑。在脉搏波时域特征集\Rnum{1}上建立的多种机器学习模型的结果均能取得较好的
识别效果。但从另一个角度而言，由于本小节使用的训练集与测试集是基本脉搏波波形划分的，同一被试的不同脉搏波波形会分别被划分至训练集与测试集。若这些波形高度相似，
就可能导致基于脉搏波波形的各

\begin{landscape}
      \zihao{-5}
      \begin{longtable}{m{1.8cm}<{\centering}m{1.8cm}<{\centering}m{1.8cm}<{\centering}m{1.8cm}<{\centering}m{1.8cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}}
            \caption{随机森林对脉搏波特征降维效果}\\
            \label{tab:rf_dr_2}\\
            \toprule
            \multicolumn{3}{c}{\textbf{随机森林特征输入}}              & \multicolumn{2}{c}{\textbf{训练时间}} & \multicolumn{2}{c}{\textbf{训练集}} & \multicolumn{5}{c}{\textbf{测试集}}                                          \\
            \textbf{贡献度比例} & \textbf{特征数量} & \textbf{数量比例} & \textbf{训练时间}  & \textbf{时间比例}  & \textbf{混淆矩阵}   & \textbf{AUC}   & \textbf{混淆矩阵} & \textbf{精确率} & \textbf{召回率} & \textbf{F1值} & \textbf{准确率} \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            \multicolumn{3}{c}{\textbf{随机森林特征输入}}              & \multicolumn{2}{c}{\textbf{训练时间}} & \multicolumn{2}{c}{\textbf{训练集}} & \multicolumn{5}{c}{\textbf{测试集}}                                          \\
            \textbf{贡献度比例} & \textbf{特征数量} & \textbf{数量比例} & \textbf{训练时间}  & \textbf{时间比例}  & \textbf{混淆矩阵}   & \textbf{AUC}   & \textbf{混淆矩阵} & \textbf{精确率} & \textbf{召回率} & \textbf{F1值} & \textbf{准确率} \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            100.0\%        & 286           & 100.0\%       & 41.30          & 100.0\%          & $\left[ \begin{array}{cc} 2404 & 141 \\ 160 & 3586 \end{array} \right]$  & 0.990        & $\left[ \begin{array}{cc} 617 & 19 \\ 28 & 909 \end{array} \right]$   & 98.0\%       & 97.0\%       & 97.5\%       & 97.0\%       \\
            90.0\%         & 162           & 56.6\%        & 38.11          & 92.3\%           & $\left[ \begin{array}{cc} 2421 & 124 \\ 134 & 3612 \end{array} \right]$  & 0.991        & $\left[ \begin{array}{cc} 619 & 17 \\ 25 & 912 \end{array} \right]$   & 98.1\%       & 97.3\%       & 97.8\%       & 97.3\%       \\
            80.0\%         & 108           & 37.8\%        & 29.33          & 71.0\%           & $\left[ \begin{array}{cc} 2431 & 114 \\ 130 & 3616 \end{array} \right]$  & 0.993        & $\left[ \begin{array}{cc} 615 & 21 \\ 26 & 911 \end{array} \right]$   & 97.7\%       & 97.2\%       & 97.5\%       & 97.0\%       \\
            70.0\%         & 74            & 25.9\%        & 23.67          & 57.3\%           & $\left[ \begin{array}{cc} 2431 & 114 \\ 125 & 3621 \end{array} \right]$  & 0.993        & $\left[ \begin{array}{cc} 618 & 18 \\ 23 & 914 \end{array} \right]$   & 98.1\%       & 97.5\%       & 97.8\%       & 97.4\%       \\
            60.0\%         & 51            & 17.8\%        & 20.91          & 50.6\%           & $\left[ \begin{array}{cc} 2442 & 103 \\ 115 & 3631 \end{array} \right]$  & 0.994        & $\left[ \begin{array}{cc} 620 & 16 \\ 27 & 910 \end{array} \right]$   & 98.3\%       & 97.1\%       & 97.7\%       & 97.3\%       \\
            50.0\%         & 34            & 11.9\%        & 15.17          & 36.7\%           & $\left[ \begin{array}{cc} 2433 & 112 \\ 146 & 3600 \end{array} \right]$  & 0.993        & $\left[ \begin{array}{cc} 616 & 20 \\ 28 & 909 \end{array} \right]$   & 97.8\%       & 97.0\%       & 97.4\%       & 97.0\%       \\
            40.0\%         & 21            & 7.3\%         & 11.90          & 28.8\%           & $\left[ \begin{array}{cc} 2432 & 113 \\ 145 & 3601 \end{array} \right]$  & 0.993        & $\left[ \begin{array}{cc} 615 & 21 \\ 30 & 907 \end{array} \right]$   & 97.7\%       & 96.8\%       & 97.3\%       & 96.8\%       \\
            30.0\%         & 13            & 4.5\%         & 9.79           & 23.7\%           & $\left[ \begin{array}{cc} 2422 & 123 \\ 200 & 3546 \end{array} \right]$  & 0.987        & $\left[ \begin{array}{cc} 612 & 24 \\ 34 & 903 \end{array} \right]$   & 97.4\%       & 96.4\%       & 96.9\%       & 96.3\%       \\
            20.0\%         & 7             & 2.4\%         & 7.82           & 18.9\%           & $\left[ \begin{array}{cc} 2277 & 268 \\ 382 & 3364 \end{array} \right]$  & 0.961        & $\left[ \begin{array}{cc} 587 & 49 \\ 92 & 845 \end{array} \right]$   & 94.5\%       & 90.1\%       & 92.3\%       & 91.0\%       \\
            10.0\%         & 3             & 1.0\%         & 5.31           & 12.9\%           & $\left[ \begin{array}{cc} 2065 & 480 \\ 484 & 3262 \end{array} \right]$  & 0.931        & $\left[ \begin{array}{cc} 539 & 97 \\ 124 & 813 \end{array} \right]$  & 89.3\%       & 86.8\%       & 88.0\%       & 86.0\%      
      \end{longtable}
\end{landscape}

\begin{landscape}
      \zihao{-5}
      \begin{longtable}{m{3cm}<{\centering}m{1.7cm}<{\centering}m{2.3cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}}
            \caption{几种机器学习模型在被试人员分层抽样的数据集上的表现}\\
            \label{tab:model_screen2}\\
            \toprule
            &  & \multicolumn{6}{c}{\textbf{训练集}} & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                      \\
            \multirow{-2}{*}{\textbf{模型类型}} & \multirow{-2}{*}{\textbf{训练时间}} & \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率} &  \textbf{AUC} &  \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率}    \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            &  & \multicolumn{6}{c}{\textbf{训练集}} & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                      \\
            \multirow{-2}{*}{\textbf{模型类型}} & \multirow{-2}{*}{\textbf{训练时间}} & \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率} &  \textbf{AUC} &  \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率}    \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            K近邻算法      &   4.08 s  &     $\left[ \begin{array}{cc} 2250 & 384 \\ 888 & 2815 \end{array} \right]$ & 88.0\% & 76.0\% &81.6\% & 80.0\% & 0.870 &
            $\left[ \begin{array}{cc} 357 & 190 \\ 141 & 839 \end{array} \right]$ & 81.5\% & 85.6\% & 83.5\% & 78.3\% \\
            决策树算法      &   1.44 s  &     $\left[ \begin{array}{cc} 2062 & 572 \\ 706 & 2997 \end{array} \right]$ & 84.0\% & 80.9\% & 82.4\% & 79.8\% & 0.862 &
            $\left[ \begin{array}{cc} 168 & 379 \\ 136 & 844 \end{array} \right]$ & 69.0\% & 86.1\% & 76.6\% & 66.3\% \\
            随机森林算法      &   50.03 s  &     $\left[ \begin{array}{cc} 2326 & 308 \\ 718 & 2985 \end{array} \right]$ & 90.6\% & 80.6\% & 85.3\% & 83.8\% & 0.929 &
            $\left[ \begin{array}{cc} 317 & 230 \\ 89 & 891 \end{array} \right]$ & 79.5\% & 90.9\% & 84.8\% & 79.1\% \\
      \end{longtable}
\end{landscape}

\begin{landscape}
      \zihao{-5}
      \begin{longtable}{m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}}
            \caption{几种机器学习模型按被试统计后的性能表现}\\
            \label{tab:model_detail}\\
            \toprule
            \multirow{2}{*}{\textbf{被试孕妇}} & \multirow{2}{*}{\textbf{波形总数}} & \multicolumn{2}{c}{\textbf{K近邻算法}} & \multicolumn{2}{c}{\textbf{决策树}} & \multicolumn{2}{c}{\textbf{随机森林}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{真实子痫前}\\ \textbf{期患病状态}\end{tabular}} \\
                        &                       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}        &                                                                        \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            \multirow{2}{*}{\textbf{被试孕妇}} & \multirow{2}{*}{\textbf{波形总数}} & \multicolumn{2}{c}{\textbf{K近邻算法}} & \multicolumn{2}{c}{\textbf{决策树}} & \multicolumn{2}{c}{\textbf{随机森林}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{真实子痫前}\\ \textbf{期患病状态}\end{tabular}} \\
                        &                       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}        &                                                                        \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            cmf                   & 88                    & 54         & 61.4\%     & 85         & 96.6\%     & 53         & 60.2\%      & 0                                                                      \\
            lxx                   & 63                    & 32         & 50.8\%     & 53         & 84.1\%     & 34         & 54.0\%      & 0                                                                      \\
            shs                   & 112                   & 37         & 33.0\%     & 105        & 93.8\%     & 55         & 49.1\%      & 0                                                                      \\
            sxh                   & 95                    & 27         & 28.4\%     & 64         & 67.4\%     & 21         & 22.1\%      & 0                                                                      \\
            wdq                   & 36                    & 0          & 0.0\%      & 0          & 0.0\%      & 0          & 0.0\%       & 0                                                                      \\
            wsj                   & 78                    & 0          & 0.0\%      & 2          & 2.6\%      & 0          & 0.0\%       & 0                                                                      \\
            ygy                   & 75                    & 40         & 53.3\%     & 70         & 93.3\%     & 67         & 89.3\%      & 0                                                                      \\
            gmn                   & 139                   & 106        & 76.3\%     & 135        & 97.1\%     & 109        & 78.4\%      & 1                                                                      \\
            ty                    & 98                    & 97         & 99.0\%     & 97         & 99.0\%     & 97         & 99.0\%      & 1                                                                      \\
            wjh                   & 86                    & 86         & 100.0\%    & 86         & 100.0\%    & 86         & 100.0\%     & 1                                                                      \\
            xjf                   & 106                   & 23         & 21.7\%     & 59         & 55.7\%     & 87         & 82.1\%      & 1                                                                      \\
            ywy                   & 111                   & 110        & 99.1\%     & 111        & 100.0\%    & 111        & 100.0\%     & 1                                                                      \\
            yxl                   & 110                   & 110        & 100.0\%    & 108        & 98.2\%     & 110        & 100.0\%     & 1                                                                      \\
            zdq                   & 89                    & 81         & 91.0\%     & 84         & 94.4\%     & 82         & 92.1\%      & 1                                                                      \\
            zl                    & 152                   & 137        & 90.1\%     & 75         & 49.3\%     & 120        & 78.9\%      & 1                                                                      \\
            zyy                   & 89                    & 89         & 100.0\%    & 89         & 100.0\%    & 89         & 100.0\%     & 1                                                                       \\    
      \end{longtable}
\end{landscape}

\leftline{类机器学习模型具有较高的识别效果。}

二、按照被试人员分层抽样

为保证在进行机器学习模型训练时同一被试的不同脉搏波数据只出现在训练集或测试集，本小节按照被试人员进行了分层抽样。通过这种方式训练得到的模型，在测试集上的表现可以用来
更加客观评估模型的泛化能力。本小节将63名被试孕妇的全部脉搏波波形被划分为训练集，余下16名被试的全波波形被划分为测试集。使用了在上小节中表现较好的K近邻、决策树及随机森林等三种算法在新划分的数据集上进行子痫前期识别模型的研究。
这三种模型的具体表现如\autoref{tab:model_screen2}所示，其中，训练集上的AUC数值是在进行了 5 层交叉验证后取得的。

横向对比\autoref{tab:model_screen}、\autoref{tab:rf_dr_2}及\autoref{tab:model_screen2}中结果可以发现，在将原始数据按被试人员分层抽样之后，K近邻、决策树及随机森林等三种算法在训练集与测试集上的均出现了
较为明显的性能下降，在测试集上性能下降得尤为明显。而单就\autoref{tab:model_screen2}而言，随机森林算法与K近邻算法的性能接近，在测试集上F1值可达到83\%以上，准确率也在78\%以上；决策树算法表现最差F1值仅有76.6\%，准确率也仅有66.3\%。
但以上数据仍旧是按照波形进行统计，并没有体现出新的分层抽样的结果。在将\autoref{tab:model_screen2}各模型对测试集上的各波形的预测结果按被试进行统计的结果如\autoref{tab:model_detail}所示。
\autoref{tab:model_detail}给出了测试集上被试人员所对应的脉搏波波形总数与其子痫前期患病状态，对三种算法分别统计了预测为子痫前期阳性的脉搏波波形数目，而预测比例则为相应模型下预测为阳性的脉搏波数目与该被人脉搏波波形总数之比。

\begin{figure}[htbp]
      \centering
      \includegraphics[width=.55\linewidth]{results/detail}
      \caption[几种机器学习的detail]{\label{fig:model_detail}几种机器学习的detail}
\end{figure}
\begin{figure}[htbp]
      \centering
      \includegraphics[width=.6\linewidth]{results/roc}
      \caption[几种机器学习ROC曲线]{\label{fig:model_roc}几种机器学习ROC曲线}
\end{figure}

若将\autoref{tab:model_detail}中三种模型的预测比例作为该模型表征子痫前期患病状态的输出，则可以得到\autoref{fig:model_detail}所示的散点图。此时，对子痫前期的识别分析可以转换为寻找
能将\autoref{fig:model_detail}进行最佳分割的阈值。遍历各模型的预测比例数值并将该数值作为分割阈值，可以得到在该数值下的混淆矩阵，并进一步得到该数值对应的敏感性与特异性。最终，三种模型的预测比例
所对应的ROC曲线如\autoref{fig:model_roc}所示。其中，决策树、K近邻及随机森林三种模型对应的AUC数值分别为0.825、0.921及0.952。同时，借助约登指数可以确定三种模型的最佳分割阈值分别为0.969、0.907与0.688，各模型在最佳阈值下的
混淆矩阵如\autoref{tab:cm_on_best}所示。三种模型的整体识别准确率分别为81.3\%、81.3\%与93.8\%。

\begin{table}[htbp]
      \zihao{5}
      \centering
      \caption{\label{tab:cm_on_best}三种模型在最佳分割阈值下的混淆矩阵}
      \begin{tabular}{ccc}
      \toprule
      \textbf{决策树算法}&\textbf{K近邻算法}&\textbf{随机森林算法}\\
      \midrule
      $\left[ \begin{array}{cc} 6 & 3 \\ 0 & 7 \end{array} \right]$ & $\left[ \begin{array}{cc} 6 & 3 \\ 0 & 7 \end{array} \right]$ & $\left[ \begin{array}{cc} 9 & 0 \\ 1 & 6 \end{array} \right]$ \\
      \bottomrule
      \end{tabular}%
\end{table}%

在上述三种分类算法中，随机森林模型无疑有着最佳的分类效果。从AUC数值而言，随机森林模型的AUC数值最大。从\autoref{tab:cm_on_best}来看，随机森林算法识别的总体准确率高于决策树与K近邻算法;
识别为假阴性的数目（0）也少于其他两种模型（3与3）。从最佳分割阈值来看，决策树与K近邻算法的预测比例分割阈值过于接近100\%，有泛化能力不足的潜在风险，而随机森林模型的分割阈值（0.688）则相对适中。

综上，本小节按照被试孕妇进行子痫前期识别模型的研究过程实际上基于子痫前期导致的病生理变化可以通过具体的脉搏波波形上，并可以通过评估该被试全部
脉搏波波形的形态特征进行群体决策的假设。本小节通过决策树算法、K近邻算法与随机森林算法分别构建了子痫前期的识别模型，并得到了各模型被测试集被试孕妇的预测比例数值。
借助AUC与ROC分析，可以分别达到81.3\%、81.3\%与93.8\%的整体准确率。

\subsection{基于脉搏波时域特征集\Rnum{2}的结果及分析}

脉搏波时域特征集\Rnum{2}实质上是将脉搏波波形各原始采样值看成是相应的输入特征进行处理。在采样率相同的情况下，脉搏波波形时长的差异会导致波形所对应的“特征”维数的差异。
参考心电分析中心率图的定义，将同一被试的所有脉搏波波形按起点进行对齐后描述在同一张图里则可以得到该被试的脉率图，如\autoref{fig:no_pe}所示。
\begin{figure}[htbp]
      \centering
      \subfigure[\label{fig:pe_hdy}患有子痫前期的被试hdy的脉率图]{
      \includegraphics[width=7.5cm]{results/hdy in group PE}
      }
      \quad
      \subfigure[\label{fig:pe_wjh}患有子痫前期的被试wjh的脉率图]{
      \includegraphics[width=7.5cm]{results/wjh in group PE}
      }
      \quad
      \subfigure[\label{fig:no_cmf}正常被试cmf的脉率图]{
      \includegraphics[width=7.5cm]{results/cmf in group No}
      }
      \quad
      \subfigure[\label{fig:no_lh}正常被试lh的脉率图]{
      \includegraphics[width=7.5cm]{results/lh in group No}
      }
      \caption{\label{fig:no_pe}被试孕妇的原始脉搏波脉率图对照}
\end{figure}

从\autoref{fig:no_pe}可以看到，在本研究采集得到的被试数据中，正常组与实验组在脉搏波波形形态及脉搏波整体分布上均有一定的差异。同时，可以看到不同脉搏波波形的时长差异较为明显。因此，本研究采取了两种
处理策略:一是在原始采样值的尾端进行补零，将所有脉搏波波形的采样点数均调整为120（该值已经可以确保涵盖本次研究采集得到的所有脉搏波波形时长的最大值）;二是
对原始采样值进行重采样，使采样点数均调整至100（该值可以保证脉搏波波形的描述的分辨率与精度），如\autoref{fig:no_pe2}所示。
在此基础上，本小节也进行了与上小节类似的研究工作，分别按波形与被试对脉搏波波形进行了处理与建模。

\begin{figure}[htbp]
      \centering
      \subfigure[\label{fig:pe_hdy2}患有子痫前期的被试hdy的脉率图]{
      \includegraphics[width=7.5cm]{results/contrast/hdy in group PE}
      }
      \quad
      \subfigure[\label{fig:pe_wjh2}患有子痫前期的被试wjh的脉率图]{
      \includegraphics[width=7.5cm]{results/contrast/wjh in group PE}
      }
      \quad
      \subfigure[\label{fig:no_cmf2}正常被试cmf的脉率图]{
      \includegraphics[width=7.5cm]{results/contrast/cmf in group No}
      }
      \quad
      \subfigure[\label{fig:no_lh2}正常被试lh的脉率图]{
      \includegraphics[width=7.5cm]{results/contrast/lh in group No}
      }
      \caption{\label{fig:no_pe2}重采样后的被试孕妇的脉搏波脉率图对照}
\end{figure}

一、按照全波波形抽样

1. 模型初筛

\begin{landscape}
      \zihao{-5}
      \begin{longtable}{m{1.5cm}<{\centering}m{1.5cm}<{\centering}m{1.5cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}}
            \caption{初筛结果}\\
            \label{tab:model_screen3}\\
            \toprule
                  & \multicolumn{1}{c}{}   & \multicolumn{1}{c}{}  & \multicolumn{6}{c}{\textbf{训练集（5层交叉验证）}}   & \multicolumn{5}{c}{\textbf{验证集}}     \\
            \multirow{-2}{*}{\textbf{处理方式}}  & \multicolumn{1}{c}{\multirow{-2}{*}{\textbf{模型类型}}} & \multicolumn{1}{c}{\multirow{-2}{*}{\textbf{训练时间}}} & \textbf{混淆矩阵} & \textbf{精确率} & \textbf{召回率}& \textbf{F1值} & \textbf{准确率}& \textbf{AUC} & \textbf{混淆矩阵}& \textbf{精确率} & \textbf{召回率} & \textbf{F1值}& \textbf{准确率} \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
                  & \multicolumn{1}{c}{}   & \multicolumn{1}{c}{}  & \multicolumn{6}{c}{\textbf{训练集（5层交叉验证）}}   & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                    \\
            \multirow{-2}{*}{\textbf{处理方式}}  & \multicolumn{1}{c}{\multirow{-2}{*}{\textbf{模型类型}}} & \multicolumn{1}{c}{\multirow{-2}{*}{\textbf{训练时间}}} & \textbf{混淆矩阵} & \textbf{精确率} & \textbf{召回率}& \textbf{F1值} & \textbf{准确率}& \textbf{AUC} & \textbf{混淆矩阵}& \textbf{精确率} & \textbf{召回率} & \textbf{F1值}& \textbf{准确率} \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            & 决策树      & 4.13    & $\left[ \begin{array}{cc} 1906 & 639 \\ 428 & 3318 \end{array} \right]$ & 83.9\%  & 88.6\%  & 86.1\% & 83.0\% & 0.914    & $\left[ \begin{array}{cc} 504 & 132 \\ 85 & 852 \end{array} \right]$ & 86.6\%  & 90.9\%  & 88.7\% & 86.2\% \\
            & K近邻     & 2.81    & $\left[ \begin{array}{cc} 2309 & 236 \\ 392 & 3354 \end{array} \right]$ & 93.4\%  & 90.0\%  & 91.4\% & 90.0\%   & 0.967  & $\left[ \begin{array}{cc} 587 & 49 \\ 87 & 850 \end{array} \right]$ & 94.5\%   & 90.7\%   & 92.6\% & 91.4\% \\
            \multirow{-3}{*}{补零} & 随机森林    & 23.23    & $\left[ \begin{array}{cc} 2295 & 250 \\ 271 & 3475 \end{array} \right]$ & 93.3\%  & 92.8\% & 93.0\% & 91.7\%  & 0.977 & $\left[ \begin{array}{cc} 587 & 49 \\ 52 & 885 \end{array} \right]$  & 94.8\% & 94.5\%   & 94.6\% & 93.6\% \\
            & 决策树      & 6.77    & $\left[ \begin{array}{cc} 2206 & 539 \\ 554 & 3192 \end{array} \right]$ & 85.6\%  & 85.2\%  & 85.4\% & 82.6\% & 0.902    & $\left[ \begin{array}{cc} 532 & 104 \\ 149 & 788 \end{array} \right]$ & 88.3\%  & 84.1\%  & 86.2\% & 83.9\% \\
            & K近邻     & 3.90    & $\left[ \begin{array}{cc} 2234 & 311 \\ 392 & 3354 \end{array} \right]$ & 91.5\%  & 89.5\%  & 90.5\% & 88.8\%   & 0.957 & $\left[ \begin{array}{cc} 583 & 53 \\ 79 & 858 \end{array} \right]$ & 94.2\%   & 91.2\%   & 92.9\% & 91.6\% \\
            \multirow{-3}{*}{重采样} & 随机森林    & 51.57    & $\left[ \begin{array}{cc} 2222 & 323 \\ 305 & 3441 \end{array} \right]$ & 91.4\%  & 91.9\% & 91.6\% & 90.0\%  & 0.967 & $\left[ \begin{array}{cc} 574 & 62 \\ 56 & 881 \end{array} \right]$  & 93.4\% & 94.0\%   & 93.7\% & 92.5\% \\
      \end{longtable}
\end{landscape}

本研究选取了在脉搏波时域特征集\Rnum{1}具有代表性的决策树、K近邻及随机森林等三种算法来进行本小节相关内容的研究。基于脉搏波时域特征集\Rnum{2}三种算法模型的具体表现如
\autoref{tab:model_screen3}所示。其中，决策树与K近邻模型的数据已经是经行过超参数调优后的结果；三种模型在训练集相关数据是对原始训练集数据经过5层交叉验证后得到的。

从\autoref{tab:model_screen3}可以得到以下结论：

\Rnum{1}、在测试集上，从模型方面来看，K近邻模型构建速度最快，随机森林处理速度最慢。而从模型分类效果来看，三种模型在测试集上的AUC数值均超过了0.900，其中随机森林的AUC数值更是在0.967以上。
在精度­召回率权衡上，K近邻与随机森林算法明显优于决策树算法，精确率、召回率及F1 数值均在 90.0\% 附近或以上。

\Rnum{2}、在验证集上，K近邻与随机森林算法的泛化能力也同样优于决策树算法。就准确率而言，K近邻与随机森林算法均在91\%以上，而决策树算法则不足87\%。
在精度­召回率权衡上，K近邻与随机森林算法明显优于决策树算法，精确率、召回率及F1 数值均在 90.0\% 。

\Rnum{3}、而横向对比两种对脉搏波波形进行对齐的处理方式可以发现，直接在脉搏波尾端进行补零处理会使各模型在构建速度、训练集AUC及模型整体准确率、精确率等方面均略微优于对脉搏波重采样处理的相应数据。

另外，对比\autoref{tab:super_para}、\autoref{tab:rf_dr_2}及\autoref{tab:model_screen3}可以发现，在脉搏波时域特征集\Rnum{1}及脉搏波时域特征集\Rnum{2}上分别构建的经过超参数调优的三种模型
在整体性能方面具有一致性。使用相同算法在两类时域特征集构建的模型在测试集上的混淆矩阵及准确率等数值接近，但整体而言，基于脉搏波时域特征集\Rnum{1}构建的模型在测试集上的泛化性能更好。这也从
侧面证明了两类时域特征集对于子痫前期的识别判断均有一定的表征能力，并且相较而言脉搏波时域特征集\Rnum{1}的表征能力更好。

2. 采样点贡献度

与上小节中处理类似，在脉搏波时域特征集\Rnum{2}上使用随机森林构建模型后，可以得到脉搏波时域特征集\Rnum{2}中各特征（即不同位置上的采样点）对最终模型的贡献度。由于基于贡献度对采样点的降维处理
缺乏实际处理的意义，本研究未进行相应的处理分析工作。\autoref{fig:rf_imp2}展示了\autoref{tab:model_screen3}中基于两种脉搏波对齐方式得到随机森林模型中各采样点的贡献度，此外，\autoref{fig:rf_imp2}
也给出了所有脉搏波波形两种对齐方式得到的“平均脉搏波”波形示意。

\begin{figure}[htbp]
      \centering
      \includegraphics[width=.6\linewidth]{results/rf_imp2}
      \caption{\label{fig:rf_imp2}脉搏波采样点对随机森林的贡献度}
\end{figure}

从\autoref{fig:rf_imp2}可以发现，按照两种方式处理得到的采样点贡献度分布形态上高度相似，均是在平均脉搏波的波峰之后出现贡献度的主峰，在平均脉搏波的下降支尾端附近出现第二个次峰。
此外，尽管补零处理的方式具有更多的采样点数（120个）,但采样点贡献度的分布更为集中，各有效特征点的贡献度数值更大。在\autoref{fig:rf_imp2}中进行补零处理的80以后的采样点几乎完全没有参与
最终随机森林模型的构建。而采样点为80则对应着本研究中得到的大多数脉搏波波形时间的最大值。因此，在逻辑上这些新值及原始采样值中采样点80以后的数据均属于冗余项。
因此，尽管在数据预处理阶段进行的是对脉搏波波形对齐进行的是补零处理插值处理，但在模型真正构建时，对待脉搏波波形对齐进行的是截断处理。

另外，结合\autoref{tab:rf_dr_1}及\autoref{fig:rf_imp2}对比两种脉搏波时域特征集上得到各特征的贡献度分布可以发现，两类时域特征集中具有子痫前期表征能力的特征出现的位置高度相似。这说明脉搏波
在波峰之后及下降支结束之前的具体形态可能是识别子痫前期的关键。

二、按照被试人员分层抽样

与上小节中按被试人员分层抽样类似，本小节在脉搏波时域特征集\Rnum{2}的数据上也采用了K 近邻、决策树及随机森林等三种算法构建了子痫前期的识别分类模型。
这三种模型的具体表现如所示。其中，训练集上的AUC数值是在进行了5层交叉验证后取得的。

\begin{landscape}
      \zihao{6}
      \begin{longtable}{m{1.5cm}<{\centering}m{1.5cm}<{\centering}m{1.5cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{2cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}m{1cm}<{\centering}}
            \caption{几种机器学习模型在被试人员分层抽样的数据集上的表现}\\
            \label{tab:model_screen4}\\
            \toprule
            &     &  & \multicolumn{6}{c}{\textbf{训练集}} & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                      \\
            \multirow{-2}{*}{\textbf{处理方式}}&\multirow{-2}{*}{\textbf{模型类型}} & \multirow{-2}{*}{\textbf{训练时间}} & \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率} &  \textbf{AUC} &  \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率}    \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            &     &  & \multicolumn{6}{c}{\textbf{训练集}} & \multicolumn{5}{c}{\textbf{验证集}}                                                                                                                                                                                                      \\
            \multirow{-2}{*}{\textbf{处理方式}}&\multirow{-2}{*}{\textbf{模型类型}} & \multirow{-2}{*}{\textbf{训练时间}} & \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率} &  \textbf{AUC} &  \textbf{混淆矩阵} &  \textbf{精确率} &  \textbf{召回率} &  \textbf{F1值} &  \textbf{准确率}    \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            &     K近邻算法      &   3.12 s  &     $\left[ \begin{array}{cc} 2191 & 443 \\ 974 & 2729 \end{array} \right]$ & 86.0\% & 73.7\% &79.4\% & 77.6\% & 0.863 &
            $\left[ \begin{array}{cc} 357 & 190 \\ 141 & 839 \end{array} \right]$ & 81.5\% & 85.6\% & 83.5\% & 78.3\% \\
            &     决策树算法      &   3.62 s  &     $\left[ \begin{array}{cc} 2056 & 578 \\ 659 & 3044 \end{array} \right]$ & 84.0\% & 82.2\% & 83.1\% & 80.5\% & 0.880 &
            $\left[ \begin{array}{cc} 202 & 345 \\ 151 & 829 \end{array} \right]$ & 70.6\% & 84.6\% & 77.0\% & 67.5\% \\
            \multirow{-3}{*}{补零}  & 随机森林算法      &   23.18 s  &     $\left[ \begin{array}{cc} 2182 & 452 \\ 684 & 3019 \end{array} \right]$ & 87.0\% & 81.5\% & 84.2\% & 82.1\% & 0.909 &
            $\left[ \begin{array}{cc} 287 & 260 \\ 132 & 848 \end{array} \right]$ & 76.5\% & 86.5\% & 81.2\% & 74.3\% \\
            &     K近邻算法      &   2.51 s  &     $\left[ \begin{array}{cc} 2251 & 483 \\ 856 & 2847 \end{array} \right]$ & 85.5\% & 76.9\% &81.0\% & 78.9\% & 0.862 &
            $\left[ \begin{array}{cc} 309 & 238 \\ 195 & 785 \end{array} \right]$ & 76.7\% & 80.1\% & 78.4\% & 71.6\% \\
            &     决策树算法      &   1.49 s  &     $\left[ \begin{array}{cc} 2039 & 595 \\ 856 & 2847 \end{array} \right]$ & 82.7\% & 76.9\% & 79.7\% & 77.1\% & 0.847 &
            $\left[ \begin{array}{cc}278 & 269 \\ 117 & 863 \end{array} \right]$ & 76.2\% & 88.1\% & 81.7\% & 74.7\% \\
            \multirow{-3}{*}{重采样}  & 随机森林算法      &   36.36 s  &     $\left[ \begin{array}{cc} 2128 & 506 \\ 822 & 2881 \end{array} \right]$ & 85.1\% & 77.8\% & 81.2\% & 79.0\% & 0.884 &
            $\left[ \begin{array}{cc} 318 & 229 \\ 122 & 858 \end{array} \right]$ & 78.9\% & 87.6\% & 83.0\% & 77.0\% \\
      \end{longtable}
\end{landscape}

\begin{landscape}
      \zihao{6}
      \begin{longtable}{m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}}
            \caption[几种机器学习模型按被试统计后的性能表现]{几种机器学习模型按被试统计后的性能表现。其中，若按两种对齐方式处理得到的预测模型对被试的预测阳性数目相差20个以上或预测比例相差15\%以上的数据栏进行了加粗显示。}\\
            \label{tab:model_detail2}\\
            \toprule
            \multirow{2}{*}{\textbf{被试孕妇}} & \multirow{2}{*}{\textbf{波形总数}} & \multicolumn{2}{c}{\textbf{K近邻算法}} & \multicolumn{2}{c}{\textbf{决策树}} & \multicolumn{2}{c}{\textbf{随机森林}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{真实子痫前}\\ \textbf{期患病状态}\end{tabular}} \\
                        &                       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}        &                                                                        \\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            \multirow{2}{*}{\textbf{被试孕妇}} & \multirow{2}{*}{\textbf{波形总数}} & \multicolumn{2}{c}{\textbf{K近邻算法}} & \multicolumn{2}{c}{\textbf{决策树}} & \multicolumn{2}{c}{\textbf{随机森林}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{真实子痫前}\\ \textbf{期患病状态}\end{tabular}} \\
                        &                       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}       & \textbf{预测阳性数目}     & \textbf{预测比例}        &                                                                        \\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            cmf                   & 88                    & 66 / 59         & 75.0\% / 67.0\%     & 75 / 60         & \textbf{85.2\% / 68.2\%}    & 61 / 49         & 69.3\% / 55.7\%      & 0                                                                      \\
            lxx                   & 63                    & 32 / 35         & 50.8\% / 55.6\%    & 50 / 38         & \textbf{79.4\% / 60.3\%}     & 37 / 42         & 58.7\% / 66.7\%     & 0                                                                      \\
            shs                   & 112                   & \textbf{32 / 58}         & \textbf{28.6\% / 51.8\%}     & 102 / 88        & 91.1\% / 78.6\%     & 57 / 47         & 50.1\% / 42.0\%      & 0                                                                      \\
            sxh                   & 95                    & 31 / 24         & 32.6\% / 25.3\%     & \textbf{52 / 27}         & \textbf{54.7\% / 28.4\%}     & 39 / 21         & \textbf{41.1\% / 22.1\%}      & 0                                                                      \\
            wdq                   & 36                    & 0 / 0          & 0.0\% / 0.0\%      & 2 / 0          & 5.6\% / 0.0\%      & 0 / 0          & 0.0\% / 0.0\%      & 0                                                                      \\
            wsj                   & 78                    & 0 / 1          & 0.0\% / 0.0\%      & 1 / 0          & 1.3\% / 0.0\%     & 0 / 0          & 0.0\% / 0.0\%     & 0                                                                      \\
            ygy                   & 75                    & 57 / 61         & 76.0\% / 81.3\%     & 63 / 56         & 84.0\% / 74.7\%    & 66 / 70         & 88.0\% / 93.3\%     & 0                                                                      \\
            gmn                   & 139                   & \textbf{51 / 82}        & \textbf{36.7\% / 59.0\%}     & 131 / 131        & 94.2\% /94.2\%    & 107 / 112        & 77.0\% / 80.6\%      & 1                                                                      \\
            ty                    & 98                    & 98 / 98         & 100.0\% / 100.0\%     & 97 / 96         & 99.0\% / 98.0\%     & 97 / 97         & 99.0\% / 99.0\%      & 1                                                                      \\
            wjh                   & 86                    & 86 / 86         & 100.0\% / 100.0\%    & 83 / 86         & 96.5\% / 100.0\%    & 86 / 86         & 100.0\% / 100.0\%    & 1                                                                      \\
            xjf                   & 106                   & \textbf{13 / 71}         & \textbf{12.3\% / 67.0\%}     & \textbf{29 / 76}         & \textbf{27.4\% / 71.7\%}    & \textbf{36 / 72}         & \textbf{34.0\% / 67.9\%}       & 1                                                                      \\
            ywy                   & 111                   & \textbf{111 / 42}        & 100.0\% / 37.8\%     & \textbf{110 / 65}        & \textbf{99.1\% / 58.6\%}    & \textbf{110 / 66}        & \textbf{99.1\% / 59.5\%}    & 1                                                                      \\
            yxl                   & 110                   & 107 / 107        & 97.3\% / 97.3\%   & 100 / 105        & 90.9\% / 95.5\%    & 108 / 110        & 98.2\% / 100.0\%     & 1                                                                      \\
            zdq                   & 89                    & 77 / 70         & 86.5\% / 78.7\%     & 78 / 76         & 87.6\% / 85.4\%     & 79 / 83         & 88.8\% / 93.3\%      & 1                                                                      \\
            zl                    & 152                   & \textbf{114 / 140}        & \textbf{75.0\% / 92.1\%}     & \textbf{112 / 139}         & \textbf{73.7\% / 91.4\%}     & 136 / 143        & 89.5\% / 94.1\%      & 1                                                                      \\
            zyy                   & 89                    & 89 / 89         & 100.0\% / 100.0\%    & 89/89         & 100.0\% / 100.0\%    & 89/89         & 100.0\% / 100.0\%    & 1                                                                       \\    
      \end{longtable}
\end{landscape}

与上小节中结论类似，对比\autoref{tab:model_screen3}与\autoref{tab:model_screen4}可以发现，在将脉搏波波形采样点按被试人员分层抽样之后，K 近邻、决策树及随机森林等三种算法在训练集与测试集上的也出现了较为明
显的性能下降，在测试集上性能下降得尤为明显。三种模型在测试集上的性能并没有表现出明显的差异，在整体准确率、精确率等具体指标上均较为接近。此外，按对脉搏波采样点方式的差异对比\autoref{tab:model_screen4}中各模型性能，可以发现
直接补零操作与重采样也未能带来较为预测模型的性能提升。而对比在脉搏波时域特征集\Rnum{1}与脉搏波时域特征集\Rnum{2}得到的分按被试分层抽样的预测结果，即\autoref{tab:model_screen2}与\autoref{tab:model_screen4}可以发现，
在训练集上与测试集上，基于两个不同的时域特征集训练得到的同种模型性能表现出了较强的一致性，从混淆矩阵到精确率等具体指标上均较为接近。这也从侧面证明了脉搏波时域特征集\Rnum{1}与脉搏波时域特征集\Rnum{2}对脉搏波的描述能力是具有一致性的。

与\autoref{tab:model_detail}类似，将\autoref{tab:model_screen4}中各模型在测试集上的预测结果按被试进行统计可以得到\autoref{tab:model_detail2}，其中，同一栏中分别显示了直接补零操作与重采样最终得到的预测为子痫前期阳性的脉搏波波形数目与比例，
有较大差异的数据进行了加粗显示。


\subsection{基于脉搏波时域特征集\Rnum{3}的结果及分析}
与前面两类直接描述脉搏波形态的特征集不同，脉搏波时域特征集\Rnum{3}定义的相关系数、弗朗明歇距离及包络面积差等三种指标旨在定量描述脉搏波间之间的差异性。
按照定义这三种指标均是对两个脉搏波波形形态差异的描述，


\begin{center}
      \zihao{5}
      \begin{longtable}{m{2.5cm}<{\centering}m{1.4cm}<{\centering}m{1.4cm}<{\centering}m{1.4cm}<{\centering}m{1.4cm}<{\centering}m{1.4cm}<{\centering}m{1.4cm}<{\centering}m{1.4cm}<{\centering}}
            \caption[脉搏波时域特征集III的显著性分析]{脉搏波时域特征集III的显著性分析。p值>0.05的特征已经加粗显示}\\
            \label{tab:feature3}\\
            \toprule
            \textbf{特征名}&\textbf{平均值}&\textbf{标准差}&\textbf{最小值}&\textbf{第25百分位数}&\textbf{第50百分位数}&\textbf{第75百分位数}&\textbf{最大值}\\
            \midrule
            \endfirsthead
            \caption[]{(续)}\\
            \midrule
            \textbf{特征名}&\textbf{平均值}&\textbf{标准差}&\textbf{最小值}&\textbf{第25百分位数}&\textbf{第50百分位数}&\textbf{第75百分位数}&\textbf{最大值}\\
            \midrule
            \endhead 
            \midrule
            \endfoot
            \bottomrule
            \endlastfoot
            相关系数    &     0.010 &     0.025 &     0.031 &     0.001 & 0.000     &     0.003 &     0.033 \\
            弗朗明歇距离    &     0.045 &    \textbf{0.109} &    \textbf{0.223} &     0.031 & 0.019     &     0.013 &     \textbf{0.104} \\
            包络面积差   &     0.005 &     0.011 &     \textbf{0.060} &     0.026 & 0.006     &     0.003 &     \textbf{0.057} \\
      \end{longtable}
\end{center}



\section{无监督学习算法的具体表现及分析}
无监督学习算法被设计用于发现未标记数据集中的隐藏结构，其期望输出是未知的。
一般而言，无监督学习的数据集仅由数据特征集$x_1,x_2,\dots,x_n$组成，没有监督学习中的目标输出，也不包含任何环境激励回报\cite{awad2015}。
与监督学习类算法相比，无监督学习算法由于不需要任何数据标签，且在训练学习时不需对原始数据的分布作过多要求，因此，无监督学习算法也逐渐称为一个新的研究热点。
聚类（clustering）是旨在发现数据间是否有潜在的相似性，无监督学习算法中最为典型的应用之一\cite{Liu2018,Li2017}。

\subsection{K均值}
K均值（K-Means）算法属于原型聚类算法的一种\cite{Zhou2016,Liu2018}。顾名思义，该算法可以将原始数据$D=\{x_1,x_2,\dots,x_n\}$划分为指定的K个簇$C=\{C_1,C_2,\dots,C_n\}$,并使得平方误差最小化
\begin{equation}
    \label{equ:leastsq}
    E=\sum_{i=1}^k \sum_{x \in C_i}{||x- \mu_i||}_2^2
\end{equation}
其中，$\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i}{x}$是簇$C_i$的均值向量。

\autoref{equ:leastsq}反应了各簇内样本整体相似度，同时也反应了簇内样本围绕均值向量的紧密程度。一般而言，$E$的数值越小，簇内样本相似度越好，此次聚类的效果越好。
\autoref{alg:kmean}给出了K均值算法的工作原理。\autoref{fig:kmeans}则给出了一个具体的使用K均值算法进行聚类的过程示意。
\begin{breakablealgorithm}
    \caption[KMeans聚类算法]{KMeans聚类算法\cite{Zhou2016}}
    \label{alg:kmean}
    \begin{algorithmic}[1] %每行显示行号
        \Require 样本集$D=\{x_1,x_2,\dots,x_m\}$，聚类簇数$k$。
        \Ensure 簇划分$C=\{C_1,C_2,\dots,C_k\}$
        \State 从D中随机选择k个样本作为初始均值向量$\{\mu_1,\mu_2,\dots,\mu_k\}$
        \Repeat
        \State 令$C_i=\emptyset (1\le i\le k)$
            \For {$j=1,2,\dots,m$}
                \State 计算样本$x_j$与各均值向量$\mu_i (1\le i \le k)$的距离：$d_{ij}={||x_j - \mu_j||}_2$
                \State 根据距离最近的均值向量确定$x_j$的簇标记：$\lambda_j = \arg \min_{i \in \{1,2,\dots,k\}} d_{ij}$
                \State 将样本$x_j$划入对应的簇：$C_{\lambda_j} = C_{\lambda_j} \cup \{x_j\}$
            \EndFor
            \For {$i=1,2,\dots,k$}
                \State 计算新均值向量：$\mu_i^{'}=\frac{1}{|C_i|} \sum_{x \in C_i}{x}$
                \If {$\mu_i^{'} \ne \mu_i$}
                    \State 将当前均值向量$\mu_i$更新为$\mu_i^{'}$
                \Else
                    \State 保持当前均值向量不变
                \EndIf
            \EndFor
        \Until 当前均值向量均未更新
    \end{algorithmic}
\end{breakablealgorithm}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{models/kmeans}
    \caption[Kmeans算法运行示意]{\label{fig:kmeans}Kmeans算法运行示意\cite{kmeans}。训练样本显示为点，簇中心向量显示为$\times$。（a）为原始数据集。（b）给出了两个随机的簇中心向量。（c-f）每两次迭代运行K均值算法后的聚类效果。}
\end{figure}

因此，我们做了额外的探究——在不给定脉搏波对应的子痫前期的数据标签的基础上，探究能否有效的将数据分成两类，并考察验证分类的效果。

本文进行了以下探究：
1.	分别使用基于ppg 特征的$ppg_feature$ 数据与基于ppg波形数据的$ppg_points $数据，使用sklearn的kmeans 方法进行了聚类分析，其中超参数$n_clusters$被设置为2。
此时，此时若我们将聚类结果作为其学习的分类结果，与数据对应的数据标签进行对照，也可以得到混淆矩阵分别如下

之所以每种分析会得到两个混淆矩阵，是因为聚类分析的簇是不带标签的。两个混淆矩阵是给予簇不同的标签（健康或子痫）。为分析方便，我们选取准确率高的一种聚类结果，即上表中的2、3。并在此基础上进行后续分析。

2.	从表格中可以看到两种数据的聚类分析的准确率分别达到了61.6\%与51.5\%。
基于特征的聚类效果优于直接使用脉搏波波形数据的。
其次，乍一看，数据分类效果并不太好。特别是后者的准确率堪堪超过50\%（典型的随机分类的效果）
但是，我们需要注意到聚类分析的本质是根据数据特征的相似度，也即数据波形的相似度。
因此，让我们来考察下划分出来的波形究竟如何。
于是，我们把图画出来。

此外，我们还需要注意到另外一个变量因素，在PE的影响下，所有患病孕妇的数据波形是会受到影响的，而与正常波形形态有异。因此，所有PE患者都受到了一定程序的医学干预（包括不限于降血压等治疗），目的是使PE患者能够恢复正常水平。因此，会出现假阴性远高于假阳性的现象。可以总结为，在PE确实能改变孕妇脉搏波波形的前提下，聚类分析中出现的假阴性高于假阳性证明是医学干预的必然结果。分析数据与理论分析保持一致。
3.	到具体波形

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth]{unsupervised/cluster using points_2d}
    \caption[]{\label{fig:cluster2d}111}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\linewidth]{unsupervised/cluster using points_3d}
    \caption[]{\label{fig:cluster3d}222}
\end{figure}
\section{小结}